# Computer Vision; correlation, edges and lines
By: Zackary T. Jorquera,
May 5, 2017

# Abstract: 
Computer vision (CV) is a process that allows for the computer to have an understanding of an image or frame of a video. In order for the computer to have a high-level understanding of the image then machine learning must be used. This allows for the computer to learn what certain things are in the image such as faces unidentified objects and more. Even without machine learning, computer vision allows for finding known objects in an image like shapes, lines, circles and even cars or other common objects. In combination with Machine learning computer vision can be used to find unknown objects which can be  then classified as something.
Current uses for computer Vision are motion tracking, object tracking/shape finding, geometric analysis, and more. Most of these will be gone over in depth. Some good examples of what computer vision is currently used in are the Xbox connect, self driving cars, Google earth’s ability to create models for buildings, and object tracking for video effects.
Computer vision started in the 1960s as an attempt to mimic the human vision system. In the 1970s some of the most common algorithms, that are still used today, were made including edge recognition and polyhedral/non-polyhedral recognition. By 1990 there were algorithms for camera location calibration, giving the computer the ability to know where its camera is relative to other coordinate schemes.
All of the code referenced[4] in this paper was made in MatLab as well as most syntax used for explaining things. Code is highlighted in gray. MatLab is a high-performance language for technical computing. It allows for computation, visualization, and programming in a simple environment using common mathematical notation.
  

# Image Representation: 
While programing an algorithm that does anything with an image, the image can not be thought of as just a picture. An image in this sense is a mathematical function with the input for the pixel location of X and Y and output of the R, G, and B (red, green, and blue). This input is what gives the term pixel its name, Picture Object. Giving the function each input and output have its own range of possible values. This is because an image is described with a function and it does not have an infinite height and width so the inputs must have a range. Where ‘R’ is the notation for a range of each input/output relating to the function of the image the total of the image can be written like:  Rx*Ry for total position and  Rr*Rg*Rb for color or RI for grayscale i.e. Rx means the range of x positions within the image. Each range does not have to start with 0 or end with some predefined value, meaning that as long as the range is defined an image could have the range for x being [10 59] where the image starts a 10 and ends at 59. Or maybe the intensity could be [0 255] or [5 7] where 0 and 5 is the value for black or no intensity respectively and 255 and 7 are for white or full intensity. Bringing it all back together an image is a collection of pixels which are described within a range for is intensity and can be retrieved using its x and y coordinates which are also within a range[1]. 
	The fourier transform[1], which is another way to represent an image, still classifies an image as a function, but instead of having pixels it is expressed by repeating signals represented by sine and cosine functions in both the X dimension and Y. The basic concept comes from the fourier transform which states that any function can be given only using sine and cosine functions. This applied to an image gives this method of showing an image. Using the fourier method gives the ability to recognize repeating patterns in the image but not the ability to know the specific pixel’s intensity. 

# Terminology: 
To understand how computer vision  works, many terms, notations and equations are used. First and most important is an array, this is a variable type that is like a spreadsheet in that it is defined by one or more dimensions. An array can store data by correlating itself with an X and Y position (for 2D arrays). 
Noise and filters: noise is a result from the fact that many images are not perfect, many have “errors” in them. These errors are referred to as noise, which includes multiple different types. The easiest type of noise to remove is called salt and pepper noise (IMG-1(cnx.org)). These single pixels can be removed by applying a median filter which retains the edges but removes out of range pixels. The second type of noise is static noise(Gaussian noise), this is like the product of adding old television static to an image (IMG-2[1]). To remove this noise a Gaussian filter is used which in turn also blurs the image losing some edges. This Gaussian filter works by iterating through every pixel and getting the weighted average of the surrounding pixels, meaning that it gets the average of the surrounding with respect to distance so that farther away pixels have less effect of the value of the pixel being calculated. The Gaussian filter is also used for blurring and shrinking images and retaining edge details in the shrunken images. This seems pretty contradictory but it’s not and it works this way because when shrinking in image the Gaussian filter helps save information about the details left out of the image when it is shrunk but also to have the basic representation.
 Polar representation of a line[2]: the geometric equation for lines is different for use in the hough space (what the hough space is is in the hough section). A line in most math classes is written with the equation y = mx + b where b is the offset (or y-intercept, its all the same thing) and m is the slope. Using hough space the the polar representation of a line is used, which is defined by theta and rho instead of m and b (IMG-3[2]). Using this representation everything is relative to the origin point at (0,0) at the top left point in the image (In computer science the origin is at the top left where x goes to the right and y goes down). Theta is the angle between the east direction and the perpendicular line from the origin to the line being represented. ‘Rho’ is the length of the perpendicular line. Why this would ever matter is in combination with hough space where it is easier to calculate all possible lines from a point. This topic will be expanded in later in the Hough section along with how hough space can be used to identify lines or circles.



           

# Basic correlation: 
Because this essay is not about correlation I’ll make it quick. Correlation is the process of taking a section of an image and then checking it against every other section of the image. For each check it captures data expressing the correlation between the two sections (one of the sections in the template and the other is the specific part the image is looking at). The best example of this in action is to find Waldo in a Where's Waldo image (IMG-4[4]). This example uses an image of waldo as the template and uses that to correlate with the image. To do this the correlation algorithm returns an array of the size of the image where 1 (white) is an exact correlation and -1(black) is no correlation for each pixel. Most of the time there is never an exact match so the program just selects the point in the array with the highest value. Using this method some questions arise: what if the template image has a different range for intercity than the image that the template is looking through or if the images were taken with different lighting? This has a fairly simple answer and this is because the algorithm normalizes both images and thus making the min and max intensity not important because they have both been turned into   Ri = [0 1]. This also allows for different shades of image to be correlated based on the relative brightness in the image, not the captured brightness. This also cause some disadvantages such as the correlation algorithm not knowing the difference between a bright colored image and one with dark colors initially. As correlation gets more advanced so does the algorithm and this leads to better, more effective methods, all using the same basic concept except now with scale, rotation, and perspective. (IMG-5[3]). This helps If the two images are taken from different angles making the perspective different.


# Edges Finding: 
This is where it starts to get complex. An edge finding algorithm takes in the input of a grayscale image and returns a logical image(every pixel is either 1 or 0) where the edges are portrayed as white pixels and everything else is black (IMG-6[6]). The leading method for finding edges is the canny method. This edge finding algorithm consists of 4 main steps[1]. First, is to apply a Gaussian filter to suppress noise in the grayscale image. Using the filtered image a gradient image can be made which shows the derivatives with respect to the intensity of the image (i.e. change in intensity through each pixel). There is also a threshold that filters out small changes in the images that might not present edges. Next the lines that remain from the thresholding are thinned to one pixel in width. Finally the lines are connected so there are less gaps between lines. This results in a edge image that can then be used for other purposes. The gradient image (array) represents each value as a vector. Because each pixel in the gradient image (it is an array) are vectors, each vector has a direction towards the most rapid increase in intensity and the magnitude or the amount of increase in intensity. The direction can be calculated with the arctangent of the x and y changes of intensity. In theory when calculating the change in intensity with derivation it is done with continuous math so you would set a coefficient representing the distance between the two points to then get the slope as low as possible using a limit, to get the most exact slope. In computer science this can not be done, the coefficient has to be pre defined. Luckily in a image, the minimum distance for change in intensity in 1 pixel so no real calculus is needed. This means that a filter can be used to get the partial derivative, and that is in for x  [-1 1] (note that the X dimension goes to the right for increase in position). This will result in a positive or negative number depending on the direction of change where greater than 0 represents a positive slope and less than 0 represents a negative slope and 0 represents no change (or the position where in function (image) is at a minimum or maximum making both sides create a total slope of zero (IMG-7)). The same thing can be done for the Y direction. More commonly used when getting the change is the sobel operator (IMG-8blog.saush.com y goes up) for the filter, this just allows for the x and y dimensions to combine better. It is important to note that because the filter gets the average it needs to be divided by 8 in order for the values to not be too bright[1]. Everything else is more in depth to get into now.


# Hough: 
The hough space uses the polar representation of a line to convert points into a line as a stepping stone. The hough space is not really its own space it is just the method used for finding possible lines for point in in image. Although i will reference images and ideas portraying the hough space as its own space just to help understand what it does. IMG-9[4] shows how a single point to transferred into hough space, it turns the point into a sinusoid (a sine wave). This happens because of how the polar representation of a line represents the all of the possible lines that intersect a point. In order for every possible line to be made off of the point the algorithm goes over every possible theta value between 0 and 180 not including 180 (the 180 degrees direction is already covered by 0 degrees). An finds the only possible rho value that makes the line intersect the point (IMG-10[4]). So for every point on the hough space in IMG-9[4] represents a possible line that could be drawn on the image intersecting the point. This is good and all but how can the program detect the places with most intersection without the use of slow complex math that reiterates for every possible combination of lines. This is bypassed by having the hough space be made up of ‘bins’ to collect ‘votes’ from each edge point[1]. This process is called a hough line accumulator. The bins are defined by creating a 2D array with sizes of 180 divided by the theta bin size. This means that when the point in the image space is being turned into the sinusoid it only goes through every possible theta value for every theta bin size. If the theta bin size is 0.5 degrees then the theta value that the program iterates through will be (0, 0.5, 1, 1.5, 2 …, 179, 179.5). There are also a rho bins that includes values from the diagonal length of the image times -1 to the diagonal length of the image([(-diagonal length) (diagonal length)]) taking in account the rho bin sizes. From here, using one of the theta values a possible line for a pixel can be made by calculating the value of rho with: rho = (x * cos(theta)) - (y * sin(theta)). This equation can be derived from trig identities/functions. Then the bin at the location of (rho, theta) is given a ‘vote’. The value of the (rho, theta) position is one value greater than it was originally (1 goes to 2 56 goes to 57 and so on). When this is done for every theta in the range and for every edge pixel it is then mapped to the hough space like IMG-11[4]. This image is in the form of a heat map where white pixels have the most ‘votes’, red has lower amounts of votes and black has none. This graph is arranged so that the y axis is rho and that theta is the x axis. 
The accumulator generates a 2D array with ‘votes’ in each ‘bin’. The next step is to count the amount of votes that each bin has. As a reminder each bin represents a line that can be graphed in image space and thus when finding which bins have the most votes (peaks), this is really like finding which line fits best with the most amount of edges. Although it works a similar way it is not how the line of best fit is calculated but rather a way to identify where possible lines are. This is done by first filtering out every ‘bin’ that does not have a certain ‘vote’ threshold of points. These bins with greater amounts of votes than the threshold are called peaks. When graphing the lines from the peaks, there usually are a maximum number of points that are wanted. This is why the list of peaks is then sorted so that the best possible lines are graphed on the image. In IMG-11[4] I have graphed a box around each peak that will be graphed as a line later. 
The last step is to draw the lines from the peaks that were outputted by the previous algorithm. This can be accomplished by the matLab function ‘plot()’ which takes in the inputs of the range that the line spans in the x direction and y direction. Because the graph of a line is traditionally written y = mx+b, the range for x is set to the image’s width (1, size(img,2)). Everything else is derived from the equation: rho = (x * cos(theta)) - (y * sin(theta)) so:                                           m = cot(theta) and b = (-1 * csc(theta) * rho). Because x is a range, when y is set to mx + b then it also becomes a range making a line on the image (IMG-12[4] where the green lines have been graphed on).
This all works fine and all except when the bin size is not one. Because the bins are part of an array and when getting a stored value at a position the values for the position can only be integer values. Because of this, when storing the values the theta and rho bin values must be stretched to start at 1 and have a separation of 1. For rho this can be accomplished with:       rho_m = int32(rhoFound/rhoBinSize+ rhoBins/2) where int32 converts the value to a 32 bit integer and rhoBins = 2*(int32(sqrt((size(img,2) - 1)^2 + (size(img,1) - 1)^2)/rhoStep)) + 1 where size(img,1) is the height of the image and size(img,2) is the width. For theta it is                     theta_m = int32(theta/thetaBinSize) + 1. I’m using the ‘_m’ to show that it was modified to int form to fit in the array. The + 1 in this and for the rhoBins is used to turn the 0 value into a 1 because in matlab the arrays position start at 1. Because the value have been change when the line is graphed the values have to be reverted back to the original values. To do this where the values have been modified the program has to keep track of that and this can be easily done by having an array with the same size as all of the possible theta values contaning all of the possible theta values for the thetaBinsSize. Meaning if the thetaBinSize is 0.5 then the array would be (0, 0.5, 1, 1.5, 2 …, 179, 179.5) in positions (1,2,3,4,5 …, 358, 359) respectively. The rho version works the same way. These two things are stored as two output variables, T and R. To use them is is a simple as T(position) and R(position) which converts from the modified values to the real values.
Now that we are done with lines we can move to circles and other shapes. In some ways finding a circle is easier because when finding a circle each point on the circle is the radius away from the center. Thus, no geometric math is needed and the instead of finding every possible line that a point can make you only need to find every point radius away from the circle. This is done with: [a b] = [int32(x - (radius * cos(theta)))    int32(y + (radius * sind(theta)))]  where ‘a’ and ‘b’ are the x and y positions of the point radius away from the edge pixel. Note that bin sizes can be changed, although because a and b represent individual pixels on the image it is unadvised to do so as it skips over pixels. Finding peaks uses the same method of finding the most voted for bins on the hough space, except instead of sinusoids intersecting, the circle for every edge point intersect (IMG-13[4]). When drawing the circle on the image it takes the peaks and draws a circle from its radius around the point (a,b) (IMG-14[4]).

# Conclusion: 
We have talked about how Computer vision  is used to do correlation, and object finding. We specifically talked about how basic correlation methods work, by using a template and systematically iterating through the possible location. We have also desscused the way an edge finding algorithm works by using a filter to get the change in intensity between pixels. And finishing that off we talked about how, using the edge image, a program can detect lines and circles.

# Acknowledgement: 
This was created for my junior year high school LA research project.

# Sources: 
[1] Bobick, Aaron, Irfan Essa, and Arpan Chakraborty. "Introduction to Computer Vision."Udacity. Georgia Institute of Technology, n.d. Web. 05 May 2017. <https://www.udacity.com/course/introduction-to-computer-vision--ud810>.

[2] "Houghlines." Hough Transform - MATLAB Hough. MathWorks, n.d. Web. 05 May 2017. <http://www.mathworks.com/help/images/ref/hough.html>.

[3] Zhao, Feng, Qingming Huang, and Wen Gao. "Image Matching by Normalized Cross-Correlation." IMAGE MATCHING BY NORMALIZED CROSS-CORRELATION. N.p., June 2006. Web. 5 May 2017. <https://www.researchgate.net/publication/224641323_Image_Matching_by_Normalized_Cross-Correlation>.

[4] Jorquera, Zack.
<https://github.com/ZackJorquera/ComputerVisionLineAndCircleFinding> 
